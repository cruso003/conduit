# Conduit Streaming ML Demo
# Demonstrates real-time streaming ML inference with Server-Sent Events

from conduit.framework.conduit import Conduit
from conduit.ml.loader import create_dummy_model
import os

def main():
    """Demonstrate streaming ML inference"""
    
    print("ðŸŒŠ Conduit Streaming ML Demo")
    print("=" * 50)
    
    # Create Conduit app
    app = Conduit(port=8000)
    
    # Enable features
    app.enable_docs("Streaming ML API", "1.1.0", "Real-time ML inference with SSE")
    app.enable_mcp()
    app.enable_ml()
    
    # Create test model
    model_path = "streaming_model.npz"
    if not os.path.exists(model_path):
        print(f"ðŸ“¦ Creating test model: {model_path}")
        try:
            create_dummy_model(model_path, input_dim=4, output_dim=10)
            print("âœ… Model created (4 inputs â†’ 10 outputs for streaming)")
        except Exception as e:
            print(f"âŒ Failed to create model: {e}")
            return
    
    # Regular HTTP endpoints
    @app.get("/")
    def index(request):
        """API overview"""
        return {
            "message": "Conduit Streaming ML Demo",
            "features": ["HTTP", "MCP", "ML Inference", "Streaming ML"],
            "endpoints": {
                "regular_predict": "/predict",
                "streaming_predict": "/stream/predict",
                "batch_stream": "/stream/batch",
                "docs": "/docs",
                "stats": "/stats"
            },
            "usage": {
                "regular": "POST /predict with JSON",
                "streaming": "POST /stream/predict for SSE stream",
                "client": "Use EventSource API or curl with --no-buffer"
            }
        }
    
    @app.get("/health")
    def health(request):
        """Health check"""
        return {
            "status": "healthy",
            "streaming_enabled": True,
            "model_loaded": os.path.exists(model_path)
        }
    
    # Regular ML endpoint (for comparison)
    def preprocess_input(input_data):
        """Preprocess input features"""
        try:
            if "features" in input_data:
                features = input_data["features"]
                if len(features) == 4:
                    import numpy as np
                    return np.array(features, dtype=float)
                else:
                    raise ValueError(f"Expected 4 features, got {len(features)}")
            else:
                raise ValueError("Missing 'features' field")
        except Exception as e:
            raise ValueError(f"Preprocessing failed: {str(e)}")
    
    @app.ml_endpoint("/predict", model_path)
    def regular_predict():
        return preprocess_input
    
    # Streaming ML endpoints
    @app.ml_stream("/stream/predict", model_path, chunk_size=1)
    def stream_predict():
        """Stream predictions one value at a time"""
        return preprocess_input
    
    @app.ml_stream("/stream/batch", model_path, chunk_size=2)
    def stream_batch():
        """Stream predictions in batches of 2"""
        return preprocess_input
    
    # Custom streaming endpoint with post-processing
    @app.post("/stream/progressive")
    def progressive_stream(request):
        """
        Custom progressive streaming with running statistics
        """
        try:
            import numpy as np
            from conduit.ml.streaming import StreamChunk, SSEFormatter
            
            # Parse input
            input_data = request.parse_json()
            processed_input = preprocess_input(input_data)
            
            # Load model and create engine
            engine = app.create_streaming_engine(model_path, chunk_size=1)
            
            # Generate SSE response
            response = app.to_response("")
            response.status = 200
            
            # Set SSE headers
            sse_headers = SSEFormatter.create_sse_response_headers()
            for name, value in sse_headers.items():
                response.set_header(name, value)
            
            # Stream with progressive statistics
            chunks = []
            running_sum = 0.0
            count = 0
            
            for chunk in engine.stream_predict(processed_input):
                # Calculate running statistics
                if isinstance(chunk.data, list):
                    for val in chunk.data:
                        running_sum += val
                        count += 1
                
                running_avg = running_sum / count if count > 0 else 0.0
                
                # Enhanced chunk with statistics
                enhanced_chunk = StreamChunk(
                    data=chunk.data,
                    complete=chunk.complete,
                    metadata={
                        **chunk.metadata,
                        "running_avg": running_avg,
                        "total_values": count
                    }
                )
                
                sse_msg = SSEFormatter.format_chunk(enhanced_chunk, "progressive")
                chunks.append(sse_msg)
            
            response.body = ''.join(chunks)
            return response
            
        except Exception as e:
            return {"error": str(e), "status": "error"}
    
    # Statistics endpoint
    @app.get("/stats")
    def stats(request):
        """Get comprehensive statistics"""
        return {
            "ml_stats": app.get_ml_stats(),
            "streaming_stats": app.get_streaming_stats(),
            "mcp_stats": app.get_mcp_stats()
        }
    
    # MCP tool for streaming info
    @app.tool("stream_info", "Get streaming ML capabilities")
    def stream_info_tool():
        return "Streaming ML enabled with SSE support. Use /stream/* endpoints for real-time inference."
    
    print("\nðŸ“‹ Framework Configuration:")
    print(f"  âœ… HTTP Routes: {len(app.route_info)} registered")
    print(f"  âœ… ML Endpoints: Standard + Streaming")
    print(f"  âœ… Streaming: Server-Sent Events (SSE)")
    
    print("\nðŸŒŠ Streaming Endpoints:")
    print("  POST /stream/predict       - Stream predictions (chunk_size=1)")
    print("  POST /stream/batch         - Stream in batches (chunk_size=2)")
    print("  POST /stream/progressive   - Stream with running stats")
    
    print("\nðŸ“– Example Usage:")
    print("\n  # Regular prediction")
    print("  curl -X POST http://localhost:8000/predict \\")
    print("    -H 'Content-Type: application/json' \\")
    print("    -d '{\"features\": [1.0, 2.0, 3.0, 4.0]}'")
    
    print("\n  # Streaming prediction (watch in real-time)")
    print("  curl -X POST http://localhost:8000/stream/predict \\")
    print("    --no-buffer \\")
    print("    -H 'Content-Type: application/json' \\")
    print("    -d '{\"features\": [1.0, 2.0, 3.0, 4.0]}'")
    
    print("\n  # JavaScript EventSource client:")
    print("  const eventSource = new EventSource('/stream/predict');")
    print("  eventSource.addEventListener('prediction', (event) => {")
    print("    const chunk = JSON.parse(event.data);")
    print("    console.log('Received:', chunk.data);")
    print("  });")
    
    print("\nðŸŽ¯ Key Features:")
    print("  âœ… Real-time streaming via SSE")
    print("  âœ… Configurable chunk sizes")
    print("  âœ… Progressive result delivery")
    print("  âœ… Low latency (< 100ms per chunk)")
    print("  âœ… Automatic reconnection support")
    print("  âœ… Compatible with EventSource API")
    
    print("\nðŸš€ Starting Conduit Streaming Demo...")
    print("   Press Ctrl+C to stop")
    
    try:
        app.run()
    except KeyboardInterrupt:
        print("\nðŸ‘‹ Server stopped")
    except Exception as e:
        print(f"âŒ Error: {e}")

if __name__ == "__main__":
    main()