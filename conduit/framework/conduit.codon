"""
Conduit Framework - Main API

The high-level framework class that provides decorator-based routing
and integrates all the low-level components.

Note: This framework integrates with the Conduit compiler plugin for
compile-time routing optimization. The plugin detects routes via
add_route_metadata() calls and generates an optimized dispatch function.

IMPORTANT: This framework REQUIRES compilation with the Conduit plugin:
    codon build -plugin conduit your_app.codon

The plugin generates the conduit_plugin_dispatch function at compile-time.
"""

from conduit.net.socket import Socket
from conduit.http.request import HTTPRequest, parse_http_request  
from conduit.http.response import HTTPResponse, not_found_json
from conduit.http.router import Router
from conduit.http.middleware import MiddlewareChain, LoggerMiddleware, CORSMiddleware, TimingMiddleware
from conduit.http.docs import APIDocGenerator, RouteDoc
from conduit.mcp.simple_http_client import SimpleAPITools
from conduit.ml.loader import BaseModel, load_model, ModelError
from conduit.ml.inference import InferenceEngine, create_inference_engine, InferenceError
from conduit.ml.streaming import StreamingInferenceEngine, StreamChunk, SSEFormatter, create_streaming_engine
from conduit.ml.pipeline import (
    MLPipeline, PipelineStage, EnsemblePredictor, 
    create_pipeline, create_ensemble, PipelineError
)
from conduit.ml.vectors import (
    VectorDB, VectorDocument, SearchResult, RAGPipeline,
    create_vector_db, create_rag_pipeline, VectorDBError
)
from conduit.ml.onnx_support import (
    ONNXModel, DeviceManager, load_onnx_model,
    is_onnx_available, has_gpu_support, ONNXError
)

# Helper function for path parameter matching (called from plugin-generated code)
def match_path_pattern(path: str, pattern: str, params: Dict[str, str]) -> bool:
    """
    Match an incoming path against a route pattern with parameters.
    
    Args:
        path: Incoming request path (e.g., "/users/123")
        pattern: Route pattern (e.g., "/users/:id")
        params: Dictionary to populate with extracted parameters
    
    Returns:
        True if path matches pattern, False otherwise
    
    Example:
        params = {}
        if match_path_pattern("/users/123", "/users/:id", params):
            print(params["id"])  # "123"
    """
    # Split both path and pattern by '/'
    path_segments = path.split('/')[1:]  # Skip first empty segment
    pattern_segments = pattern.split('/')[1:]
    
    # Must have same number of segments
    if len(path_segments) != len(pattern_segments):
        return False
    
    # Match segment by segment
    for i in range(len(path_segments)):
        pattern_seg = pattern_segments[i]
        path_seg = path_segments[i]
        
        if pattern_seg.startswith(':'):
            # Parameter segment - extract value
            param_name = pattern_seg[1:]  # Remove ':' prefix
            params[param_name] = path_seg
        else:
            # Static segment - must match exactly
            if pattern_seg != path_seg:
                return False
    
    return True

# Default 404 handler - used by plugin when routes don't match
def _conduit_default_404(path: str = "") -> HTTPResponse:
    """Default 404 handler called by plugin-generated dispatch"""
    return not_found_json(path)

# Plugin replaces this stub's body at compile-time
def conduit_plugin_dispatch(method: str, path: str, request: HTTPRequest) -> HTTPResponse:
    # This body will be replaced by the plugin
    # If you see this message, the plugin didn't run!
    return HTTPResponse(500, "ERROR: Plugin did not generate dispatch function")

# Plugin metadata function - intercepted by compiler plugin
def conduit_dispatch_bucketed(method: str, path: str, request: HTTPRequest) -> HTTPResponse:
    """
    Dispatch function that routes requests to handlers
    
    This function calls the plugin-generated optimized dispatch.
    When compiled with -plugin conduit, routes to the plugin version.
    Without the plugin, returns an error.
    """
    return __plugin_dispatch_bucketed__(method, path, request)

# Plugin metadata function - intercepted by compiler plugin
def add_route_metadata(method: str, path: str, handler_name: str):
    """
    Route registration metadata for compiler plugin
    
    This function is called by decorators to register route metadata.
    The Conduit compiler plugin intercepts these calls during compilation
    to generate optimized dispatch code.
    
    Args:
        method: HTTP method (GET, POST, etc.)
        path: Route pattern (/users/:id)
        handler_name: Name of the handler function
    """
    # Empty implementation - plugin intercepts the IR call
    pass

class RouteInfo:
    """Information about a registered route"""
    pattern: str
    method: str
    name: str
    
    def __init__(self, pattern: str, method: str, name: str = ""):
        self.pattern = pattern
        self.method = method
        self.name = name

class MCPTool:
    """Information about a registered MCP tool"""
    name: str
    description: str
    handler_func: Optional[Callable]
    input_schema: Optional[dict]
    
    def __init__(self, name: str, description: str = "", handler_func: Optional[Callable] = None):
        self.name = name
        self.description = description  
        self.handler_func = handler_func
        self.input_schema = None

class MLEndpoint:
    """Information about a registered ML inference endpoint"""
    pattern: str
    model: BaseModel
    engine: InferenceEngine
    preprocessor: Optional[Callable]
    postprocessor: Optional[Callable]
    
    def __init__(self, pattern: str, model: BaseModel):
        self.pattern = pattern
        self.model = model
        self.engine = InferenceEngine(model)
        self.preprocessor = None
        self.postprocessor = None
    
    def set_preprocessor(self, func: Callable):
        """Set preprocessing function"""
        self.preprocessor = func
        self.engine.set_preprocessor(func)
    
    def set_postprocessor(self, func: Callable): 
        """Set postprocessing function"""
        self.postprocessor = func
        self.engine.set_postprocessor(func)
    
    def predict(self, input_data: Any) -> Any:
        """Run prediction through the inference engine"""
        return self.engine.predict(input_data)

class MCPServer:
    """Integrated MCP server for the framework"""
    
    def __init__(self):
        """Initialize MCP server."""
        self.tools = []
        self.api_tools = SimpleAPITools()
        self.total_requests = 0
        self.total_token_savings = 0
    
    def register_tool(self, tool: MCPTool):
        """Register a new MCP tool."""
        self.tools.append(tool)
    
    def optimize_json(self, json_str: str) -> str:
        """Apply token optimization to JSON response."""
        optimized = json_str.replace(" ", "")
        optimized = optimized.replace('"jsonrpc":', '"j":')
        optimized = optimized.replace('"result":', '"r":')
        optimized = optimized.replace('"content":', '"c":')
        optimized = optimized.replace('"type":"text"', '"t":"txt"')
        optimized = optimized.replace('"text":', '"tx":')
        return optimized
    
    def handle_mcp_request(self, request_data: str) -> str:
        """Handle MCP JSON-RPC request."""
        self.total_requests += 1
        
        try:
            # Simple JSON-RPC parsing
            if '"method":"tools/list"' in request_data:
                return self._handle_list_tools(request_data)
            elif '"method":"tools/call"' in request_data:
                return self._handle_tool_call(request_data)
            elif '"method":"initialize"' in request_data:
                return self._handle_initialize(request_data)
            else:
                return '{"jsonrpc":"2.0","id":1,"error":{"code":-32601,"message":"Method not found"}}'
        
        except Exception as e:
            return '{"jsonrpc":"2.0","id":1,"error":{"code":-32000,"message":"Server error"}}'
    
    def _extract_id(self, request_data: str) -> int:
        """Extract request ID from JSON-RPC."""
        if '"id":' in request_data:
            start = request_data.find('"id":') + 5
            end = start
            while end < len(request_data) and request_data[end].isdigit():
                end += 1
            if end > start:
                return int(request_data[start:end])
        return 1
    
    def _handle_list_tools(self, request_data: str) -> str:
        """Handle tools/list request."""
        request_id = self._extract_id(request_data)
        
        # Build tools list
        tools_json = '{"tools":['
        
        for i, tool in enumerate(self.tools):
            if i > 0:
                tools_json += ','
            tools_json += '{"name":"' + tool.name + '","description":"' + tool.description + '"}'
        
        tools_json += ']}'
        
        response = '{"jsonrpc":"2.0","id":' + str(request_id) + ',"result":' + tools_json + '}'
        return self.optimize_json(response)
    
    def _handle_tool_call(self, request_data: str) -> str:
        """Handle tools/call request.""" 
        request_id = self._extract_id(request_data)
        
        # Extract tool name (simple string parsing)
        tool_name = ""
        if '"name":"' in request_data:
            start = request_data.find('"name":"') + 8
            end = request_data.find('"', start)
            if end > start:
                tool_name = request_data[start:end]
        
        # Find and execute tool
        for tool in self.tools:
            if tool.name == tool_name:
                try:
                    if tool.handler_func:
                        # Call the tool handler
                        result = tool.handler_func()  # Simplified - no args for now
                        response_json = '{"content":[{"type":"text","text":"' + str(result) + '"}]}'
                        response = '{"jsonrpc":"2.0","id":' + str(request_id) + ',"result":' + response_json + '}'
                        
                        # Apply token optimization
                        original_length = len(response)
                        optimized = self.optimize_json(response)
                        self.total_token_savings += (original_length - len(optimized))
                        
                        return optimized
                except Exception as e:
                    pass
        
        # Tool not found or error
        error_response = '{"jsonrpc":"2.0","id":' + str(request_id) + ',"error":{"code":-32000,"message":"Tool not found or error"}}'
        return self.optimize_json(error_response)
    
    def _handle_initialize(self, request_data: str) -> str:
        """Handle initialize request."""
        request_id = self._extract_id(request_data)
        init_response = '{"jsonrpc":"2.0","id":' + str(request_id) + ',"result":{"protocolVersion":"2024-11-05","serverInfo":{"name":"conduit-mcp","version":"1.0.0"}}}'
        return self.optimize_json(init_response)

class Conduit:
    """
    Main framework class for building web applications
    
    Example:
        app = Conduit()
        
        @app.get("/")
        def index(request):
            return {"message": "Hello, World!"}
        
        app.run()
    """
    
    host: str
    port: int
    router: Router
    route_info: List[RouteInfo]  # Metadata about routes
    middleware_chain: MiddlewareChain  # Middleware chain
    running: bool
    _dispatch_fn_set: bool  # Flag to track if dispatch is configured
    
    # Auto-documentation fields
    docs_enabled: bool
    doc_generator: APIDocGenerator
    api_title: str
    api_version: str
    
    # ML inference fields
    ml_endpoints: List[MLEndpoint]
    ml_enabled: bool
    
    def __init__(self, host: str = "0.0.0.0", port: int = 8000):
        """Initialize Conduit application"""
        self.host = host
        self.port = port
        self.router = Router()
        self.route_info = []
        self.middleware_chain = MiddlewareChain()
        self.running = False
        self._dispatch_fn_set = False
        self.docs_enabled = False
        self.api_title = "API"
        self.api_version = "1.0.0"
        # MCP integration
        self.mcp_server = None
        self.mcp_enabled = False
        # ML inference integration
        self.ml_endpoints = []
        self.ml_enabled = False
    
    def add_route_metadata(self, pattern: str, method: str, name: str = ""):
        """Register route metadata (called by decorators)"""
        # Register with runtime router for fallback
        self.router.add_route(pattern, method)
        self.route_info.append(RouteInfo(pattern, method, name))
    
    def get(self, pattern: str):
        """
        Decorator for GET routes
        
        Example:
            @app.get("/users")
            def get_users(request):
                return {"users": []}
        """
        def decorator(handler):
            # Call global metadata function (plugin intercepts this)
            add_route_metadata("GET", pattern, handler.__name__)
            
            # Register with runtime router
            self.add_route_metadata(pattern, "GET", handler.__name__)
            return handler
        return decorator
    
    def post(self, pattern: str):
        """Decorator for POST routes"""
        def decorator(handler):
            # Call global metadata function (plugin intercepts this)
            add_route_metadata("POST", pattern, handler.__name__)
            
            # Register with runtime router
            self.add_route_metadata(pattern, "POST", handler.__name__)
            return handler
        return decorator
    
    def put(self, pattern: str):
        """Decorator for PUT routes"""
        def decorator(handler):
            # Call global metadata function (plugin intercepts this)
            add_route_metadata("PUT", pattern, handler.__name__)
            
            # Register with runtime router
            self.add_route_metadata(pattern, "PUT", handler.__name__)
            return handler
        return decorator
    
    def delete(self, pattern: str):
        """Decorator for DELETE routes"""
        def decorator(handler):
            # Call global metadata function (plugin intercepts this)
            add_route_metadata("DELETE", pattern, handler.__name__)
            
            # Register with runtime router
            self.add_route_metadata(pattern, "DELETE", handler.__name__)
            return handler
        return decorator
    
    def patch(self, pattern: str):
        """Decorator for PATCH routes"""
        def decorator(handler):
            # Call global metadata function (plugin intercepts this)
            add_route_metadata("PATCH", pattern, handler.__name__)
            
            # Register with runtime router
            self.add_route_metadata(pattern, "PATCH", handler.__name__)
            return handler
        return decorator
    
    def use(self, middleware):
        """
        Register middleware to process requests/responses
        
        Example:
            from conduit.http.middleware_simple import logger_middleware, cors_middleware
            
            app.use(logger_middleware())
            app.use(cors_middleware("*"))
        """
        # Dispatch to appropriate add method based on type
        if isinstance(middleware, LoggerMiddleware):
            self.middleware_chain.add_logger(middleware)
        elif isinstance(middleware, CORSMiddleware):
            self.middleware_chain.add_cors(middleware)
        elif isinstance(middleware, TimingMiddleware):
            self.middleware_chain.add_timing(middleware)
    
    def enable_docs(self, title: str = "API Documentation", version: str = "1.0.0", 
                    description: str = "API built with Conduit"):
        """
        Enable auto-documentation with Swagger UI
        
        Serves interactive API docs at /docs and OpenAPI spec at /openapi.json
        
        Args:
            title: API title
            version: API version
            description: API description
            
        Example:
            app = Conduit()
            app.enable_docs(title="My API", version="2.0.0")
            app.run()
            # Visit http://localhost:8000/docs
        """
        self.docs_enabled = True
        self.api_title = title
        self.api_version = version
        self.doc_generator = APIDocGenerator(title, version)
        self.doc_generator.description = description
        
        print(f"ðŸ“š Auto-documentation enabled:")
        print(f"   Swagger UI: http://{self.host}:{self.port}/docs")
        print(f"   OpenAPI spec: http://{self.host}:{self.port}/openapi.json")
    
    def match_route(self, request: HTTPRequest):
        """
        Match a route and extract parameters
        
        Returns (matched: bool, route_idx: int, params: Dict[str, str])
        """
        matched, route_idx, params = self.router.match(request.path, request.method)
        
        if matched:
            request.params = params
        
        return (matched, route_idx, params)
    
    def print_dispatch_code(self):
        """
        Print dispatch boilerplate code for copy-paste
        
        Due to Codon's type system limitations, we cannot store arbitrary
        function pointers in collections. This helper generates the manual
        dispatch code you need to add to your server loop.
        
        Call this after registering all routes to see the code you need.
        """
        num_routes = len(self.route_info)
        
        print("")
        print("=" * 70)
        print(f"Dispatch Code for {num_routes} Routes")
        print("=" * 70)
        print("")
        print("Copy this into your server loop (after match_route):")
        print("")
        print("if matched:")
        
        for i in range(num_routes):
            route = self.route_info[i]
            if i == 0:
                print(f"    if route_idx == {i}:  # {route.method} {route.pattern}")
            else:
                print(f"    elif route_idx == {i}:  # {route.method} {route.pattern}")
            print(f"        response = app.to_response(handler_{i}(request))")
        
        print("    else:")
        print("        response = app.error_response(\"Unknown route index\")")
        print("else:")
        print("    response = app.not_found_response()")
        print("")
        print("=" * 70)
        print("")
        print("Handler mapping:")
        for i in range(num_routes):
            route = self.route_info[i]
            print(f"  handler_{i} = {route.name}  # {route.method} {route.pattern}")
        print("")
    
    def not_found_response(self) -> HTTPResponse:
        """Generate 404 response"""
        response = HTTPResponse(404, '{"error": "Route not found"}')
        response.set_content_type("application/json")
        return response
    
    def error_response(self, error_msg: str) -> HTTPResponse:
        """Generate 500 error response"""
        json_msg = '{"error": "' + error_msg + '"}'
        response = HTTPResponse(500, json_msg)
        response.set_content_type("application/json")
        return response
    
    def to_response(self, result) -> HTTPResponse:
        """
        Convert handler return value to HTTPResponse
        
        Supports:
        - HTTPResponse (pass through)
        - Dict (convert to JSON)
        - Tuple[Dict, int] (JSON with status code)
        - str (plain text)
        """
        # Already an HTTPResponse
        if isinstance(result, HTTPResponse):
            return result
        
        # Dict - convert to JSON
        if isinstance(result, dict):
            json_str = self._dict_to_json(result)
            response = HTTPResponse(200, json_str)
            response.set_content_type("application/json")
            return response
        
        # Tuple of (dict, status_code)
        if isinstance(result, tuple) and len(result) == 2:
            data, status = result
            if isinstance(data, dict):
                json_str = self._dict_to_json(data)
                response = HTTPResponse(status, json_str)
                response.set_content_type("application/json")
                return response
        
        # String - plain text
        if isinstance(result, str):
            response = HTTPResponse(200, result)
            response.set_content_type("text/plain")
            return response
        
        # Fallback - convert to string
        response = HTTPResponse(200, str(result))
        response.set_content_type("text/plain")
        return response
    
    def _dict_to_json(self, data: Dict) -> str:
        """
        Convert dict to JSON string
        
        Simple implementation - handle common cases
        """
        pairs = []
        for key in data:
            value = data[key]
            
            # Format value based on type
            if isinstance(value, str):
                value_str = '"' + value + '"'
            elif isinstance(value, int):
                value_str = str(value)
            elif isinstance(value, float):
                value_str = str(value)
            elif isinstance(value, bool):
                value_str = "true" if value else "false"
            elif isinstance(value, list):
                # Simple list handling
                value_str = self._list_to_json(value)
            elif isinstance(value, dict):
                # Nested dict
                value_str = self._dict_to_json(value)
            else:
                value_str = '"' + str(value) + '"'
            
            pair = '"' + key + '": ' + value_str
            pairs.append(pair)
        
        return '{' + ', '.join(pairs) + '}'
    
    def _list_to_json(self, items: List) -> str:
        """Convert list to JSON array string"""
        json_items = []
        for item in items:
            if isinstance(item, str):
                json_items.append('"' + item + '"')
            elif isinstance(item, dict):
                json_items.append(self._dict_to_json(item))
            else:
                json_items.append(str(item))
        
        return '[' + ', '.join(json_items) + ']'
    
    def handle_request(self, request: HTTPRequest) -> HTTPResponse:
        """
        Handle incoming HTTP request with middleware
        
        Routes the request to the appropriate handler and applies middleware.
        Middleware can modify responses (add headers, log, etc).
        
        Args:
            request: Parsed HTTP request
            
        Returns:
            HTTP response
        """
        # Check for MCP requests first
        if self.mcp_enabled and request.path == "/mcp" and request.method == "POST":
            # Handle MCP JSON-RPC request
            body = request.body if hasattr(request, 'body') else ""
            mcp_response = self.mcp_server.handle_mcp_request(body)
            
            # Create HTTP response
            response = HTTPResponse(200)
            response.body = mcp_response
            response.set_header("Content-Type", "application/json")
            response.set_header("Content-Length", str(len(mcp_response)))
            return response
        
        # Check for ML inference endpoints
        if self.ml_enabled:
            for ml_endpoint in self.ml_endpoints:
                if request.path == ml_endpoint.pattern and request.method == "POST":
                    try:
                        # Parse JSON input
                        if hasattr(request, 'body') and request.body:
                            input_data = request.parse_json()
                        else:
                            error_response = HTTPResponse(400)
                            error_response.body = '{"error": "No input data provided", "status": "error"}'
                            error_response.set_header("Content-Type", "application/json")
                            return error_response
                        
                        # Run prediction
                        result = ml_endpoint.predict(input_data)
                        
                        # Create success response
                        response_data = {
                            "prediction": result,
                            "model": ml_endpoint.model.get_info()["name"],
                            "status": "success"
                        }
                        
                        response_body = self.to_json_string(response_data)
                        
                        response = HTTPResponse(200)
                        response.body = response_body
                        response.set_header("Content-Type", "application/json")
                        response.set_header("Content-Length", str(len(response_body)))
                        return response
                        
                    except ModelError as e:
                        error_response = HTTPResponse(500)
                        error_data = {"error": f"Model error: {e.message}", "status": "error"}
                        error_body = self.to_json_string(error_data)
                        error_response.body = error_body
                        error_response.set_header("Content-Type", "application/json")
                        return error_response
                        
                    except InferenceError as e:
                        error_response = HTTPResponse(500)
                        error_data = {"error": f"Inference error: {e.message}", "status": "error"}
                        error_body = self.to_json_string(error_data)
                        error_response.body = error_body
                        error_response.set_header("Content-Type", "application/json")
                        return error_response
                        
                    except Exception as e:
                        error_response = HTTPResponse(500)
                        error_data = {"error": f"Unexpected error: {str(e)}", "status": "error"}
                        error_body = self.to_json_string(error_data)
                        error_response.body = error_body
                        error_response.set_header("Content-Type", "application/json")
                        return error_response
        
        # Check for auto-docs routes
        if self.docs_enabled:
            if request.path == "/docs":
                return self.serve_docs()
            elif request.path == "/openapi.json":
                return self.serve_openapi_json()
        
        # Call plugin dispatch to get response
        try:
            response = conduit_plugin_dispatch(request.method, request.path, request)
        except:
            # Fallback for unmatched routes (404)
            response = not_found_json(request.path)
        
        # Apply middleware post-processing
        response = self.middleware_chain.execute_post_process(request, response)
        
        return response
    
    def serve_docs(self) -> HTTPResponse:
        """Serve Swagger UI HTML for interactive API documentation"""
        if not self.docs_enabled:
            return not_found_json("/docs")
        
        # Generate OpenAPI spec from registered routes
        self._populate_docs()
        
        # Get branded Swagger UI HTML
        html = self.doc_generator.generate_swagger_html()
        
        response = HTTPResponse(200)
        response.body = html
        response.set_header("Content-Type", "text/html; charset=utf-8")
        response.set_header("Content-Length", str(len(html)))
        return response
    
    def serve_openapi_json(self) -> HTTPResponse:
        """Serve OpenAPI 3.0 JSON specification"""
        if not self.docs_enabled:
            return not_found_json("/openapi.json")
        
        # Generate OpenAPI spec from registered routes
        self._populate_docs()
        
        # Get OpenAPI JSON
        spec_json = self.doc_generator.generate_openapi_json()
        
        response = HTTPResponse(200)
        response.body = spec_json
        response.set_header("Content-Type", "application/json")
        response.set_header("Content-Length", str(len(spec_json)))
        return response
    
    def _populate_docs(self):
        """Populate doc_generator with route information"""
        # Clear existing routes
        self.doc_generator.routes = []
        
        # Add documentation for each registered route
        for route_info in self.route_info:
            route_doc = RouteDoc(route_info.pattern, route_info.method)
            route_doc.set_summary(f"{route_info.method} {route_info.pattern}")
            
            # Use route name if available, otherwise generic description
            if route_info.name:
                route_doc.set_description(f"Handler: {route_info.name}")
            else:
                route_doc.set_description(f"Endpoint: {route_info.pattern}")
            
            # Extract path parameters
            if ":" in route_info.pattern:
                parts = route_info.pattern.split("/")
                for part in parts:
                    if part.startswith(":"):
                        param_name = part[1:]
                        route_doc.add_param(param_name, "string")
            
            self.doc_generator.add_route(route_doc)
    
    def _get_status_text(self, status: int) -> str:
        """Get status text for status code"""
        status_texts = {
            200: "OK",
            201: "Created",
            204: "No Content",
            400: "Bad Request",
            401: "Unauthorized",
            403: "Forbidden",
            404: "Not Found",
            500: "Internal Server Error",
            502: "Bad Gateway",
            503: "Service Unavailable"
        }
        
        if status in status_texts:
            return status_texts[status]
        return "Unknown"
    
    # MCP Integration Methods
    
    def enable_mcp(self, path: str = "/mcp"):
        """
        Enable MCP (Model Context Protocol) support
        
        Args:
            path: URL path for MCP endpoint (default: "/mcp")
        """
        if self.mcp_server is None:
            self.mcp_server = MCPServer()
        
        self.mcp_enabled = True
        
        # Register MCP endpoint
        def mcp_handler(request):
            if request.method == "POST":
                # Handle MCP JSON-RPC request
                body = request.body if hasattr(request, 'body') else ""
                return self.mcp_server.handle_mcp_request(body)
            else:
                return {"error": "MCP endpoint only accepts POST requests"}
        
        # Add MCP route
        self.add_route_metadata(path, "POST", "mcp_handler")
    
    def tool(self, name: str, description: str = ""):
        """
        Decorator to register a function as an MCP tool
        
        Args:
            name: Tool name for MCP protocol
            description: Tool description
            
        Example:
            @app.tool("get_weather", "Get current weather for a location")
            def weather_tool():
                return "Sunny, 72Â°F"
        """
        def decorator(handler):
            if self.mcp_server is None:
                self.mcp_server = MCPServer()
            
            # Create MCP tool
            tool = MCPTool(name, description, handler)
            self.mcp_server.register_tool(tool)
            
            return handler
        return decorator
    
    def get_mcp_stats(self) -> Dict:
        """
        Get MCP server statistics
        
        Returns:
            Dictionary with request count and token savings
        """
        if self.mcp_server is None:
            return {"requests": 0, "token_savings": 0, "tools": 0}
        
        return {
            "requests": self.mcp_server.total_requests,
            "token_savings": self.mcp_server.total_token_savings,
            "tools": len(self.mcp_server.tools)
        }

    # ML Inference Integration
    def enable_ml(self):
        """
        Enable ML inference capabilities
        
        This enables the framework to serve machine learning models
        through dedicated ML endpoints.
        """
        self.ml_enabled = True
        print("ML inference enabled")

    def ml_endpoint(self, pattern: str, model_path: str):
        """
        Decorator to register a machine learning inference endpoint
        
        Args:
            pattern: URL pattern for the ML endpoint (e.g., "/predict")
            model_path: Path to the model file
            
        Example:
            @app.ml_endpoint("/predict", "model.npz")
            def preprocess(input_data):
                return np.array(input_data["features"])
        """
        def decorator(preprocessor=None):
            try:
                # Load the model
                model = load_model(model_path)
                
                # Create ML endpoint
                ml_endpoint = MLEndpoint(pattern, model)
                
                # Set preprocessor if provided
                if preprocessor is not None:
                    ml_endpoint.set_preprocessor(preprocessor)
                
                # Register the endpoint
                self.ml_endpoints.append(ml_endpoint)
                
                # Register as HTTP POST route for predictions
                def ml_handler(request):
                    try:
                        # Parse JSON input
                        if hasattr(request, 'body') and request.body:
                            input_data = request.parse_json()
                        else:
                            return {"error": "No input data provided"}
                        
                        # Run prediction
                        result = ml_endpoint.predict(input_data)
                        
                        # Return prediction result
                        return {
                            "prediction": result,
                            "model": model.get_info()["name"],
                            "status": "success"
                        }
                        
                    except ModelError as e:
                        return {"error": f"Model error: {e.message}", "status": "error"}
                    except InferenceError as e:
                        return {"error": f"Inference error: {e.message}", "status": "error"}
                    except Exception as e:
                        return {"error": f"Unexpected error: {str(e)}", "status": "error"}
                
                # Register the HTTP route
                self.add_route_metadata(pattern, "POST", f"ml_endpoint_{len(self.ml_endpoints)}")
                
                # Enable ML if not already enabled
                if not self.ml_enabled:
                    self.enable_ml()
                
                print(f"Registered ML endpoint: POST {pattern} -> {model_path}")
                
                return preprocessor if preprocessor is not None else ml_handler
                
            except Exception as e:
                print(f"Failed to register ML endpoint {pattern}: {str(e)}")
                return None
                
        return decorator

    def post_ml(self, pattern: str):
        """
        Alternative decorator for ML endpoints that expect the model to be provided separately
        
        Example:
            model = load_model("classifier.npz") 
            
            @app.post_ml("/classify")
            def classify_handler(request, prediction):
                # prediction is the raw model output
                class_idx = prediction.argmax()
                confidence = float(prediction.max())
                return {
                    "class": CLASSES[class_idx],
                    "confidence": confidence
                }
        """
        def decorator(handler):
            # This is a placeholder for now - would need more complex integration
            # to pass model predictions to the handler
            print(f"Registered ML post-processing endpoint: POST {pattern}")
            self.add_route_metadata(pattern, "POST", handler.__name__)
            return handler
        return decorator

    def get_ml_stats(self) -> Dict:
        """
        Get ML inference statistics
        
        Returns:
            Dictionary with ML endpoint information and inference statistics
        """
        if not self.ml_enabled:
            return {"ml_enabled": False, "endpoints": 0}
        
        endpoint_stats = []
        total_predictions = 0
        total_errors = 0
        
        for endpoint in self.ml_endpoints:
            stats = endpoint.engine.get_stats()
            endpoint_stats.append({
                "pattern": endpoint.pattern,
                "model": stats["model_info"],
                "predictions": stats["inference_count"],
                "errors": stats["error_count"],
                "avg_time_ms": stats["avg_time_ms"],
                "predictions_per_sec": stats["predictions_per_sec"]
            })
            total_predictions += stats["inference_count"]
            total_errors += stats["error_count"]
        
        return {
            "ml_enabled": True,
            "endpoints": len(self.ml_endpoints),
            "total_predictions": total_predictions,
            "total_errors": total_errors,
            "error_rate": total_errors / max(1, total_predictions + total_errors),
            "endpoint_stats": endpoint_stats
        }

    def load_model(self, model_path: str, cache: bool = True) -> BaseModel:
        """
        Load a machine learning model
        
        Args:
            model_path: Path to model file
            cache: Whether to use model cache (default: True)
            
        Returns:
            Loaded model instance
        """
        return load_model(model_path, force_reload=not cache)

    def create_ml_engine(self, model_path: str) -> InferenceEngine:
        """
        Create an inference engine for a model
        
        Args:
            model_path: Path to model file
            
        Returns:
            Configured inference engine
        """
        return create_inference_engine(model_path)

    # Streaming ML Inference
    def ml_stream(self, pattern: str, model_path: str, chunk_size: int = 1):
        """
        Decorator for streaming ML inference endpoints
        
        Args:
            pattern: URL pattern for the streaming endpoint (e.g., "/stream/predict")
            model_path: Path to the model file
            chunk_size: Number of predictions per chunk
            
        Example:
            @app.ml_stream("/stream/generate", "generator.npz", chunk_size=1)
            def preprocess_for_streaming(input_data):
                return np.array(input_data["features"])
        """
        def decorator(preprocessor=None):
            try:
                # Load the model
                model = load_model(model_path)
                
                # Create streaming engine
                streaming_engine = create_streaming_engine(model, chunk_size)
                
                # Register as SSE endpoint
                def streaming_handler(request):
                    try:
                        # Parse input
                        if hasattr(request, 'body') and request.body:
                            input_data = request.parse_json()
                        else:
                            return {"error": "No input data provided", "status": "error"}
                        
                        # Preprocess if function provided
                        if preprocessor is not None:
                            processed_input = preprocessor(input_data)
                        else:
                            processed_input = input_data
                        
                        # Create SSE response
                        response = HTTPResponse(200)
                        
                        # Set SSE headers
                        sse_headers = SSEFormatter.create_sse_response_headers()
                        for header_name, header_value in sse_headers.items():
                            response.set_header(header_name, header_value)
                        
                        # Generate streaming response
                        chunks = []
                        for chunk in streaming_engine.stream_predict(processed_input):
                            sse_message = SSEFormatter.format_chunk(chunk, "prediction")
                            chunks.append(sse_message)
                        
                        # Combine all SSE messages
                        response.body = ''.join(chunks)
                        
                        return response
                        
                    except Exception as e:
                        # Return error as SSE
                        error_chunk = StreamChunk(
                            data={"error": str(e)},
                            complete=True,
                            metadata={"error": True}
                        )
                        error_sse = SSEFormatter.format_chunk(error_chunk, "error")
                        
                        response = HTTPResponse(500)
                        sse_headers = SSEFormatter.create_sse_response_headers()
                        for header_name, header_value in sse_headers.items():
                            response.set_header(header_name, header_value)
                        response.body = error_sse
                        
                        return response
                
                # Register the HTTP route
                self.add_route_metadata(pattern, "POST", f"ml_stream_{len(self.ml_endpoints)}")
                
                print(f"Registered streaming ML endpoint: POST {pattern} -> {model_path}")
                
                return preprocessor if preprocessor is not None else streaming_handler
                
            except Exception as e:
                print(f"Failed to register streaming ML endpoint {pattern}: {str(e)}")
                return None
                
        return decorator
    
    def create_streaming_engine(self, model_path: str, chunk_size: int = 1) -> StreamingInferenceEngine:
        """
        Create a streaming inference engine
        
        Args:
            model_path: Path to model file
            chunk_size: Predictions per chunk
            
        Returns:
            Configured StreamingInferenceEngine
        """
        model = load_model(model_path)
        return create_streaming_engine(model, chunk_size)

    def get_streaming_stats(self) -> Dict:
        """
        Get streaming inference statistics
        
        Returns:
            Dictionary with streaming statistics across all endpoints
        """
        # This would need to track streaming engines globally
        # For now, return placeholder
        return {
            "streaming_enabled": self.ml_enabled,
            "message": "Streaming statistics tracking to be implemented"
        }
    
    # ========== ML Pipeline Methods ==========
    
    def create_pipeline(self, name: str = "pipeline") -> MLPipeline:
        """
        Create a new ML pipeline
        
        Args:
            name: Pipeline name
            
        Returns:
            Empty MLPipeline ready for stage additions
            
        Example:
            pipeline = app.create_pipeline("text_analysis")
            pipeline.add_stage(...)
        """
        return MLPipeline(name)
    
    def create_pipeline_builder(self, name: str = "pipeline"):
        """
        Create a pipeline using fluent builder pattern
        
        Args:
            name: Pipeline name
            
        Returns:
            PipelineBuilder for chaining
            
        Example:
            pipeline = app.create_pipeline_builder("nlp") \\
                .add_model("models/tokenizer.npy", "tokenize") \\
                .add_model("models/sentiment.npy", "sentiment") \\
                .build()
        """
        return create_pipeline(name)
    
    def create_ensemble(self, model_paths: List[str], strategy: str = "average") -> EnsemblePredictor:
        """
        Create an ensemble predictor from multiple models
        
        Args:
            model_paths: List of paths to model files
            strategy: Aggregation strategy ('average', 'vote', 'max', 'min')
            
        Returns:
            EnsemblePredictor configured with given models
            
        Example:
            ensemble = app.create_ensemble([
                "models/model1.npy",
                "models/model2.npy",
                "models/model3.npy"
            ], strategy="vote")
        """
        return create_ensemble(model_paths, strategy)
    
    def ml_pipeline(self, pattern: str, pipeline: MLPipeline, method: str = "POST"):
        """
        Decorator for ML pipeline endpoints
        
        Args:
            pattern: URL pattern
            pipeline: Configured MLPipeline
            method: HTTP method (default: POST)
            
        Returns:
            Decorator function
            
        Example:
            pipeline = app.create_pipeline("text_processing")
            pipeline.add_stage(...)
            
            @app.ml_pipeline("/api/process", pipeline)
            def process_text(request):
                data = parse_json(request.body)
                return pipeline.execute(data["input"])
        """
        def decorator(handler):
            def pipeline_handler(req: HTTPRequest) -> HTTPResponse:
                try:
                    # Execute handler to get input data
                    input_data = handler(req)
                    
                    # Execute pipeline
                    result = pipeline.execute(input_data)
                    
                    # Get pipeline stats
                    stats = pipeline.get_stats()
                    
                    # Return result with metadata
                    response_data = {
                        "result": result,
                        "pipeline": {
                            "name": pipeline.name,
                            "stages": stats["num_stages"],
                            "executions": stats["pipeline_executions"]
                        }
                    }
                    
                    import json
                    return HTTPResponse(200, json.dumps(response_data))
                    
                except PipelineError as e:
                    import json
                    error_response = {
                        "error": "Pipeline execution failed",
                        "message": e.message
                    }
                    return HTTPResponse(500, json.dumps(error_response))
                except Exception as e:
                    import json
                    error_response = {
                        "error": "Internal error",
                        "message": str(e)
                    }
                    return HTTPResponse(500, json.dumps(error_response))
            
            # Register route
            self.route(pattern, method)(pipeline_handler)
            return handler
        
        return decorator
    
    def ml_ensemble(self, pattern: str, ensemble: EnsemblePredictor, method: str = "POST"):
        """
        Decorator for ensemble prediction endpoints
        
        Args:
            pattern: URL pattern
            ensemble: Configured EnsemblePredictor
            method: HTTP method (default: POST)
            
        Returns:
            Decorator function
            
        Example:
            ensemble = app.create_ensemble([
                "models/model1.npy",
                "models/model2.npy"
            ], strategy="average")
            
            @app.ml_ensemble("/api/predict", ensemble)
            def predict(request):
                data = parse_json(request.body)
                return data["features"]
        """
        def decorator(handler):
            def ensemble_handler(req: HTTPRequest) -> HTTPResponse:
                try:
                    # Execute handler to get input data
                    input_data = handler(req)
                    
                    # Get ensemble prediction
                    prediction = ensemble.predict(input_data)
                    
                    # Get stats
                    stats = ensemble.get_stats()
                    
                    # Return result with metadata
                    response_data = {
                        "prediction": prediction,
                        "ensemble": {
                            "num_models": stats["num_models"],
                            "strategy": stats["strategy"],
                            "total_predictions": stats["predictions"]
                        }
                    }
                    
                    import json
                    return HTTPResponse(200, json.dumps(response_data))
                    
                except PipelineError as e:
                    import json
                    error_response = {
                        "error": "Ensemble prediction failed",
                        "message": e.message
                    }
                    return HTTPResponse(500, json.dumps(error_response))
                except Exception as e:
                    import json
                    error_response = {
                        "error": "Internal error",
                        "message": str(e)
                    }
                    return HTTPResponse(500, json.dumps(error_response))
            
            # Register route
            self.route(pattern, method)(ensemble_handler)
            return handler
        
        return decorator
    
    # ========== Vector Database Methods ==========
    
    def create_vector_db(self, name: str = "vectordb", metric: str = "cosine") -> VectorDB:
        """
        Create a new in-memory vector database
        
        Args:
            name: Database name
            metric: Distance metric ('cosine', 'euclidean', 'manhattan', 'dot')
            
        Returns:
            VectorDB instance
            
        Example:
            db = app.create_vector_db("knowledge_base", metric="cosine")
            db.add("doc1", embedding, {"text": "..."})
        """
        return create_vector_db(name, metric)
    
    def create_rag_pipeline(self, vector_db: VectorDB, top_k: int = 3) -> RAGPipeline:
        """
        Create a RAG (Retrieval-Augmented Generation) pipeline
        
        Args:
            vector_db: Vector database for retrieval
            top_k: Number of documents to retrieve per query
            
        Returns:
            RAGPipeline instance
            
        Example:
            db = app.create_vector_db("docs")
            rag = app.create_rag_pipeline(db, top_k=5)
            results = rag.retrieve(query_embedding)
        """
        return create_rag_pipeline(vector_db, top_k)
    
    def vector_search(self, pattern: str, vector_db: VectorDB, method: str = "POST"):
        """
        Decorator for vector search endpoints
        
        Args:
            pattern: URL pattern
            vector_db: VectorDB instance
            method: HTTP method (default: POST)
            
        Returns:
            Decorator function
            
        Example:
            db = app.create_vector_db("docs")
            
            @app.vector_search("/api/search", db)
            def search(request):
                data = parse_json(request.body)
                query_vector = np.array(data["vector"])
                top_k = data.get("top_k", 5)
                return {"query_vector": query_vector, "top_k": top_k}
        """
        def decorator(handler):
            def search_handler(req: HTTPRequest) -> HTTPResponse:
                try:
                    # Execute handler to get search parameters
                    params = handler(req)
                    
                    query_vector = params["query_vector"]
                    top_k = params.get("top_k", 5)
                    filter_fn = params.get("filter_fn")
                    
                    # Perform search
                    results = vector_db.search(query_vector, top_k, filter_fn)
                    
                    # Format results
                    response_data = {
                        "results": [r.to_dict() for r in results],
                        "count": len(results),
                        "database": vector_db.name,
                        "metric": vector_db.metric
                    }
                    
                    import json
                    return HTTPResponse(200, json.dumps(response_data))
                    
                except VectorDBError as e:
                    import json
                    error_response = {
                        "error": "Vector search failed",
                        "message": e.message
                    }
                    return HTTPResponse(500, json.dumps(error_response))
                except Exception as e:
                    import json
                    error_response = {
                        "error": "Internal error",
                        "message": str(e)
                    }
                    return HTTPResponse(500, json.dumps(error_response))
            
            # Register route
            self.route(pattern, method)(search_handler)
            return handler
        
        return decorator
    
    def rag_endpoint(self, pattern: str, rag_pipeline: RAGPipeline, method: str = "POST"):
        """
        Decorator for RAG (Retrieval-Augmented Generation) endpoints
        
        Args:
            pattern: URL pattern
            rag_pipeline: RAGPipeline instance
            method: HTTP method (default: POST)
            
        Returns:
            Decorator function
            
        Example:
            db = app.create_vector_db("knowledge")
            rag = app.create_rag_pipeline(db, top_k=3)
            
            @app.rag_endpoint("/api/rag", rag)
            def rag_query(request):
                data = parse_json(request.body)
                query_vector = np.array(data["query_vector"])
                query_text = data.get("query_text", "")
                return {"query_vector": query_vector, "query_text": query_text}
        """
        def decorator(handler):
            def rag_handler(req: HTTPRequest) -> HTTPResponse:
                try:
                    # Execute handler to get query parameters
                    params = handler(req)
                    
                    query_vector = params["query_vector"]
                    query_text = params.get("query_text", "")
                    filter_fn = params.get("filter_fn")
                    
                    # Retrieve relevant documents
                    results = rag_pipeline.retrieve(query_vector, filter_fn)
                    
                    # Build context from results
                    context = rag_pipeline.build_context(results)
                    
                    # Augment query if text provided
                    augmented_query = ""
                    if query_text:
                        augmented_query = rag_pipeline.augment_query(query_text, context)
                    
                    # Format response
                    response_data = {
                        "context": context,
                        "augmented_query": augmented_query,
                        "retrieved_documents": [r.to_dict() for r in results],
                        "count": len(results)
                    }
                    
                    import json
                    return HTTPResponse(200, json.dumps(response_data))
                    
                except VectorDBError as e:
                    import json
                    error_response = {
                        "error": "RAG query failed",
                        "message": e.message
                    }
                    return HTTPResponse(500, json.dumps(error_response))
                except Exception as e:
                    import json
                    error_response = {
                        "error": "Internal error",
                        "message": str(e)
                    }
                    return HTTPResponse(500, json.dumps(error_response))
            
            # Register route
            self.route(pattern, method)(rag_handler)
            return handler
        
        return decorator

    def run(self, host: str = "", port: int = 0):
        """
        Start the application server
        
        Args:
            host: Override host (default: use constructor value)
            port: Override port (default: use constructor value)
        """
        # Use override or default
        if host:
            self.host = host
        if port > 0:
            self.port = port
        
        # Create socket
        sock = Socket()
        sock.bind(self.host, self.port)
        sock.listen(128)
        self.running = True
        
        print("")
        print("======================================================================")
        print("Conduit Framework")
        print("======================================================================")
        print("")
        print("Server running at http://" + self.host + ":" + str(self.port))
        print("")
        print("Registered routes:")
        # Skip route printing to avoid type error
        print("  " + str(len(self.router.routes)) + " route(s) registered")
        print("")
        print("Press Ctrl+C to stop")
        print("")
        
        # Accept loop
        while self.running:
            client = sock.accept()
            
            # Read request
            data = client.recv(4096)
            if len(data) == 0:
                client.close()
                continue
            
            # Parse HTTP request
            request = parse_http_request(data)
            
            # Handle request
            response = self.handle_request(request)
            
            # Send response
            response_str = response.to_bytes()
            client.send(response_str)
            client.close()
        
        sock.close()
        print("Server stopped.")
