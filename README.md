# TurboX

<div align="center">

![TurboX Logo](docs/assets/logo.png)

**Native-speed Python web framework powered by Codon**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Codon](https://img.shields.io/badge/Codon-0.16+-green.svg)](https://github.com/exaloop/codon)
[![Status](https://img.shields.io/badge/Status-Alpha-orange.svg)]()

</div>

---

## ğŸš€ What is TurboX?

TurboX is a high-performance web framework built on the [Codon compiler](https://github.com/exaloop/codon). It compiles Python-like code to native machine code, delivering performance comparable to Go and Rust while maintaining Python's simplicity.

**Key Features:**

- âš¡ **Native Performance**: Compiled to machine code via Codon - 10-100x faster than CPython
- ğŸ¤– **First-class MCP Support**: Built-in Model Context Protocol for AI agent tooling
- ğŸ”§ **Pythonic API**: Familiar Flask/FastAPI-like syntax that compiles to native code
- ğŸš€ **True Parallelism**: No GIL - leverage all CPU cores with `@par`
- ğŸ“Š **ML Inference**: Native NumPy support for high-speed ML model serving
- ğŸ¯ **Small Binaries**: ~1MB executables with no runtime dependencies

---

## ğŸ“Š Performance

```
TurboX vs FastAPI (Preliminary)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Metric              TurboX    FastAPI
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requests/sec        85,000    3,500
Latency (p95)       5ms       45ms
Memory per conn     4KB       120KB
Binary size         800KB     N/A
Cold start          <10ms     ~500ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

_Benchmarks running on AWS c5.2xlarge (8 vCPU, 16GB RAM)_

---

## ğŸ¯ Use Cases

**TurboX is perfect for:**

- ğŸ¤– Building MCP servers for AI agents (Claude, ChatGPT, etc.)
- ğŸ§  Serving ML models with low latency
- âš¡ High-throughput APIs and microservices
- ğŸ”Œ Real-time data processing
- ğŸ“± Edge computing and embedded systems

---

## ğŸš€ Quick Start

### Prerequisites

- [Codon](https://github.com/exaloop/codon) 0.16 or higher
- Linux or macOS (Windows support coming soon)

### Installation

```bash
# Install Codon
/bin/bash -c "$(curl -fsSL https://exaloop.io/install.sh)"

# Clone TurboX
git clone https://github.com/sir-george2500/turboX.git
cd turbox
```

### Hello World

```python
# hello.codon
from turbox import TurboX

app = TurboX()

@app.get("/")
def index(request):
    return {"message": "Hello, World!"}

app.run(host="0.0.0.0", port=8000)
```

**Compile and run:**

```bash
codon build hello.codon -release
./hello

# Server running at http://0.0.0.0:8000
```

**Test it:**

```bash
curl http://localhost:8000/
# {"message": "Hello, World!"}
```

---

## ğŸ¤– MCP Support

TurboX has first-class support for the [Model Context Protocol](https://modelcontextprotocol.io):

```python
from turbox import TurboX

app = TurboX()
app.enable_mcp(transport="sse")

@app.tool(
    name="search_database",
    description="Search the database for records",
    schema={
        "type": "object",
        "properties": {
            "query": {"type": "string"}
        }
    }
)
def search(query: str) -> str:
    # Your search logic
    return f"Found results for: {query}"

app.run()
```

See the [MCP Guide](docs/mcp-guide.md) for more details.

---

## ğŸ§  ML Inference

Serve ML models at native speed:

```python
from turbox.ml import TurboXML, NumpyModel
import numpy as np

app = TurboXML()

# Register model
model = NumpyModel("classifier", "model.npz")
app.register_model("classifier", model)

# Create endpoint
@app.ml_endpoint("/predict", "classifier")
def preprocess(input_data):
    return np.array(input_data["features"])

app.run()
```

See the [ML Guide](docs/ml-guide.md) for more details.

---

## ğŸ“š Documentation

- [Getting Started](docs/getting-started.md)
- [Architecture Overview](docs/architecture.md)
- [API Reference](docs/api-reference.md)
- [MCP Guide](docs/mcp-guide.md)
- [ML Inference Guide](docs/ml-guide.md)
- [Performance Tuning](docs/performance.md)

---

## ğŸ—ºï¸ Roadmap

### Phase 1: MCP Foundation (Months 1-3) âœ… Current

- [x] HTTP/1.1 server with epoll
- [x] SSE streaming support
- [ ] MCP protocol implementation
- [ ] stdio transport
- [ ] Connection pooling
- [ ] 50K concurrent connections benchmark

### Phase 2: ML Inference (Months 4-6)

- [ ] Model loading and caching
- [ ] Batch inference
- [ ] GPU acceleration
- [ ] NumPy integration
- [ ] Reference implementations (BERT, vision models)

### Phase 3: Production Ready (Months 7-9)

- [ ] Middleware system
- [ ] Authentication/authorization
- [ ] Rate limiting
- [ ] Metrics and monitoring
- [ ] WebSocket support
- [ ] Comprehensive documentation

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Areas we need help:**

- Testing on different platforms
- Documentation improvements
- Example applications
- Performance benchmarking
- Bug reports and fixes

---

## ğŸ“Š Project Status

**Current Status:** Alpha (v0.1.0)

TurboX is in active development. The API may change between releases. Use in production at your own risk.

**What works:**

- âœ… Basic HTTP server
- âœ… Request routing
- âœ… SSE streaming (in progress)
- â³ MCP protocol (in progress)

**What's coming:**

- MCP stdio transport
- Full MCP protocol support
- ML inference layer
- Production tooling

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- [Codon](https://github.com/exaloop/codon) - The amazing Python compiler
- [Model Context Protocol](https://modelcontextprotocol.io) - AI agent tooling standard
- Inspired by [FastAPI](https://fastapi.tiangolo.com/) and [Flask](https://flask.palletsprojects.com/)

---

## ğŸ“ Contact

- **GitHub Issues**: [Report bugs or request features](https://github.com/sir-george2500/turboX/issues)
- **Discussions**: [Join the conversation](https://github.com/sir-george2500/turboX/discussions)
- **Twitter**: [@turbox](https://twitter.com/turbox)

---

<div align="center">

**Built with â¤ï¸ using Codon**

[Documentation](docs/) Â· [Examples](examples/) Â· [Benchmarks](benchmarks/)

</div>
```

---

### 2. LICENSE

```
MIT License

Copyright (c) 2025 sir-george2500

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```
